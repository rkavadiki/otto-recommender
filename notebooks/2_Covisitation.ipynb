{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3082bcab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8453ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4caa9e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 146 files, in groups of 5 and chunks of 25.\n"
     ]
    }
   ],
   "source": [
    "#data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "files = glob.glob(os.path.join(data_dir,'*_pqt_chunks/*'))\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad282c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type_weight = {0:1, 1:6, 2:3}\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6/DISK_PIECES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd0d0b",
   "metadata": {},
   "source": [
    "There are 3 blocks of covisation prepartion. since this involves a cross join and the data is enormous, I have broken it down into different chunks so that we dont run out of memory.\n",
    "1. for any adid - figure out in the next 24 hrs, what the 40 adids that the user either clicked, carted and ordered, to merge to a given list of most frequent adids, follow the type_weights as above.\n",
    "2. For all the carted and ordered items, figure out in the next 24 hours, which are the other adids that the user carted or ordered, to find the most common 40, let's treat both carted and ordered weights as same(=1).\n",
    "3. Time weighted - same as point 1, but instead use time weighting, i.e, time user clicked on next adid - orginal adid clicked time / (max timestamp - min timestamp //of the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c3425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , "
     ]
    }
   ],
   "source": [
    "#preparing next 40 adids, which the user clicked or carted or ordered weighted based on the weights\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        for k in range(a,b,READ_CT):\n",
    "            df = [pd.read_parquet(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append(pd.read_parquet(files[k+i]))\n",
    "            df = pd.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "            \n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    \n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)\n",
    "    tmp.to_parquet(os.path.join(data_dir,f'top_40_carts_orders_{PART}.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559a4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preparing next 40 adids, filtered on carted and ordered adids only, considering the weight to be 1\n",
    "for j in range(6):\n",
    "    a = j*CHUNK\n",
    "    b = min( (j+1)*CHUNK, len(files) )\n",
    "    print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "\n",
    "    # => INNER CHUNKS\n",
    "    for k in range(a,b,READ_CT):\n",
    "        # READ FILE\n",
    "        df = [pd.read_parquet(files[k])]\n",
    "        for i in range(1,READ_CT): \n",
    "            if k+i<b: df.append(pd.read_parquet(files[k+i]) )\n",
    "        df = pd.concat(df,ignore_index=True,axis=0)\n",
    "        df = df.loc[df['type'].isin([1,2])] # IMPORTANT ONLY WANT CARTS AND ORDERS\n",
    "        df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "        # USE TAIL OF SESSION\n",
    "        df = df.reset_index(drop=True)\n",
    "        df['n'] = df.groupby('session').cumcount()\n",
    "        df = df.loc[df.n<30].drop('n',axis=1)\n",
    "        # CREATE PAIRS\n",
    "        df = df.merge(df,on='session')\n",
    "        df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "        # ASSIGN WEIGHTS\n",
    "        df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "        df['wgt'] = 1\n",
    "        df = df[['aid_x','aid_y','wgt']]\n",
    "        df.wgt = df.wgt.astype('float32')\n",
    "        df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "        # COMBINE INNER CHUNKS\n",
    "        if k==a: tmp2 = df\n",
    "        else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "        print(k,', ',end='')\n",
    "    print()\n",
    "    # COMBINE OUTER CHUNKS\n",
    "    if a==0: tmp = tmp2\n",
    "    else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "    del tmp2, df\n",
    "    gc.collect()\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "tmp = tmp.reset_index()\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "tmp = tmp.loc[tmp.n<40].drop('n',axis=1)\n",
    "# SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "tmp.to_parquet(os.path.join(data_dir,'top_40_buy2buy.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing next 40 adids, which the user clicked or carted or ordered, considering time decay.\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "\n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [pd.read_parquet(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append(pd.read_parquet(files[k+i]) )\n",
    "            df = pd.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_parquet(os.path.join(data_dir,f'top_40_clicks_{PART}.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d4716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
